import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import io

# --- ç¶²é è¨­å®š ---
st.set_page_config(page_title="å°è‚¡ ETF ç¸¾æ•ˆè‡ªå‹•åˆ†æ", layout="wide")

# --- æ¨™é¡Œèˆ‡æ›´æ–°æŒ‰éˆ•å€å¡Š ---
col1, col2 = st.columns([8, 1])

with col1:
    st.title("ğŸ“Š å°è‚¡ ETF ç¸¾æ•ˆåˆ†ææ’è¡Œæ¦œ")
    st.caption("è³‡æ–™ä¾†æºï¼šHiStock | è‡ªå‹•éæ¿¾ï¼šæ§“æ¡¿ã€åå‘ã€ä¸­åœ‹/æ¸¯è‚¡å¸‚å ´")

with col2:
    # é€™è£¡å°±æ˜¯ä½ è¦çš„å¼·åˆ¶æ›´æ–°æŒ‰éˆ•
    if st.button('ğŸ”„ æ›´æ–°'):
        st.cache_data.clear() # æ¸…é™¤å¿«å–
        st.rerun() # é‡æ–°åŸ·è¡Œç¶²é 

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- åŠŸèƒ½ï¼šåˆ†æå–®ä¸€æª” ETF ---
def get_etf_return(stock_code):
    url = f"https://histock.tw/stock/{stock_code}"
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ‰¾å°‹ç¸¾æ•ˆè¡¨æ ¼
        table = soup.find('table', class_='tbPerform')
        if not table: return None
        
        target_periods = {'ä¸€å­£': None, 'åŠå¹´': None, 'ä¸€å¹´': None}
        rows = table.find_all('tr')
        
        for row in rows:
            th = row.find('th')
            td = row.find('td')
            if th and td:
                p_name = th.text.strip()
                if p_name in target_periods:
                    val_span = td.find('span')
                    if val_span:
                        val_str = val_span.text.replace('%', '').replace('+', '').replace(',', '').strip()
                        try:
                            target_periods[p_name] = float(val_str)
                        except: continue
                        
        if all(v is not None for v in target_periods.values()):
            avg_return = sum(target_periods.values()) / 3
            # å˜—è©¦æŠ“å–åç¨±
            name_tag = soup.find('h3') 
            stock_name = name_tag.text.split('(')[0].strip() if name_tag else "æœªçŸ¥"
            
            return {
                'ä»£è™Ÿ': stock_code, 
                'åç¨±': stock_name,
                'ä¸€å­£%': target_periods['ä¸€å­£'], 
                'åŠå¹´%': target_periods['åŠå¹´'],
                'ä¸€å¹´%': target_periods['ä¸€å¹´'], 
                'ç¶œåˆå¹³å‡%': round(avg_return, 2)
            }
    except: pass
    return None

# --- â˜… æ ¸å¿ƒåŠŸèƒ½ï¼šæŠ“å–èˆ‡åˆ†æ (åŠ ä¸Šå¿«å–) ---
# ttl=3600 ä»£è¡¨é€™ä»½è³‡æ–™æœƒè¢«å¿«å– 1 å°æ™‚
@st.cache_data(ttl=3600, show_spinner="æ­£åœ¨æ›´æ–° ETF è³‡æ–™ä¸­ï¼Œè«‹ç¨å€™...")
def fetch_all_etf_data():
    url = "https://histock.tw/stock/etf.aspx"
    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        etf_codes = []
        rows = soup.find_all('tr')
        china_keywords = ['ä¸­åœ‹', 'ä¸Šè­‰', 'æ»¬', 'æ·±', 'æ’ç”Ÿ', 'A50', 'é¦™æ¸¯', 'æ¸¯è‚¡']

        # 1. æŠ“å–æ¸…å–®
        for row in rows:
            link = row.find('a', href=True)
            if not link or '/stock/' not in link['href']: continue
            
            href_code = link['href'].split('/')[-1]
            row_text = row.text.strip()
            
            # éæ¿¾é‚è¼¯
            if len(href_code) < 4 or len(href_code) > 6 or not href_code[0].isdigit(): continue
            if href_code.upper().endswith(('L', 'R')): continue # æ’é™¤æ§“æ¡¿/åå‘
            if any(kw in row_text for kw in china_keywords): continue # æ’é™¤ä¸­åœ‹å¸‚å ´
            
            if href_code not in etf_codes: 
                etf_codes.append(href_code)

        # 2. é–‹å§‹åˆ†æ
        results = []
        total = len(etf_codes)
        
        # å»ºç«‹é€²åº¦æ¢å®¹å™¨
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, code in enumerate(etf_codes):
            # æ›´æ–°é€²åº¦æ–‡å­—
            status_text.text(f"ğŸš€ æ­£åœ¨åˆ†æ [{i+1}/{total}]: {code} ...")
            progress_bar.progress((i + 1) / total)
            
            data = get_etf_return(code)
            if data:
                results.append(data)
            time.sleep(0.05) #ç¨å¾®åŠ å¿«é€Ÿåº¦
        
        # æ¸…é™¤é€²åº¦æ¢
        status_text.empty()
        progress_bar.empty()
        
        return pd.DataFrame(results)

    except Exception as e:
        st.error(f"è³‡æ–™æŠ“å–å¤±æ•—: {e}")
        return pd.DataFrame()

# --- ç¶²é åŸ·è¡Œæµç¨‹ ---

# å‘¼å«ä¸»å‡½å¼ (å¦‚æœæœ‰å¿«å–å°±è®€å¿«å–ï¼Œæ²’æœ‰å°±é‡è·‘)
df_final = fetch_all_etf_data()

if not df_final.empty:
    # æ’åºï¼šç¶œåˆå¹³å‡ç”±é«˜åˆ°ä½
    df_sorted = df_final.sort_values(by='ç¶œåˆå¹³å‡%', ascending=False).reset_index(drop=True)
    
    # é¡¯ç¤ºæˆåŠŸè¨Šæ¯ (æ³¨æ„é€™è£¡çš„æ‹¬è™Ÿè¦å°å¿ƒ)
    st.success(f"âœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼å…±åˆ†æ {len(df_sorted)} æª” ETFã€‚")
    
    # é¡¯ç¤ºæ’è¡Œæ¦œè¡¨æ ¼
    st.dataframe(df_sorted, use_container_width=True)
    
    # æº–å‚™ Excel ä¸‹è¼‰
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_sorted.to_excel(writer, index=False)
    
    # ä¸‹è¼‰æŒ‰éˆ• (é€™å°±æ˜¯æœ€å®¹æ˜“å‡ºéŒ¯çš„åœ°æ–¹ï¼Œè«‹ç¢ºä¿è¤‡è£½å®Œæ•´)
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰ Excel åˆ†æå ±è¡¨",
        data=output.getvalue(),
        file_name="ETF_Analysis_Report.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

else:
    st.warning("ç›®å‰æ²’æœ‰æŠ“åˆ°è³‡æ–™ï¼Œè«‹é»æ“Šå³ä¸Šè§’çš„ã€Œæ›´æ–°ã€æŒ‰éˆ•é‡è©¦ã€‚")
